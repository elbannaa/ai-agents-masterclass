# Phase 1: Self-Hosted AI Automation Stack (Mac M1 | Docker | VS Code)
# Environment: LOCALHOST (SSL and domain will be enabled in Phase 3)
# Base Stack: coleam00/local-ai-packaged + Whisper (ASR) + Piper (TTS) + Ollama (LLM)

version: "3.9"

services:
  n8n:
    image: n8nio/n8n
    container_name: n8n
    ports:
      - "${N8N_PORT}:${N8N_PORT}"
    env_file:
      - .env
    environment:
      - N8N_BASIC_AUTH_ACTIVE=${N8N_BASIC_AUTH_ACTIVE}
      - N8N_BASIC_AUTH_USER=${N8N_BASIC_AUTH_USER}
      - N8N_BASIC_AUTH_PASSWORD=${N8N_BASIC_AUTH_PASSWORD}
      - N8N_HOST=${N8N_HOST}
      - N8N_PORT=${N8N_PORT}
      - WEBHOOK_TUNNEL_URL=${WEBHOOK_TUNNEL_URL}
      - GENERIC_TIMEZONE=${GENERIC_TIMEZONE}
    volumes:
      - n8n_data:/home/node/.n8n
    # Future: Traefik labels for reverse proxy with SSL
    # labels:
    #   - "traefik.enable=true"
    #   - "traefik.http.routers.n8n.rule=Host(`promethus-oracle.com`)"
    #   - "traefik.http.routers.n8n.entrypoints=websecure"
    #   - "traefik.http.routers.n8n.tls.certresolver=myresolver"
    #   - "traefik.http.services.n8n.loadbalancer.server.port=5678"

  postgres:
    image: postgres:14
    restart: always
    env_file:
      - .env
    environment:
      - POSTGRES_USER=${POSTGRES_USER}
      - POSTGRES_PASSWORD=${POSTGRES_PASSWORD}
      - POSTGRES_DB=${POSTGRES_DB}
    volumes:
      - postgres_data:/var/lib/postgresql/data

  ollama:
    image: ollama/ollama
    container_name: ollama
    ports:
      - "11434:11434"
    volumes:
      - ollama_data:/root/.ollama
    # Optional future addition: mount models directory for custom models

  qdrant:
    image: qdrant/qdrant
    container_name: qdrant
    ports:
      - "6333:6333"
    volumes:
      - qdrant_storage:/qdrant/storage
    # Ready for use as vector DB in RAG or n8n LangChain agent flows

  whisper-api:
    image: ghcr.io/openai/whisper-asr-webservice:latest
    container_name: whisper-api
    ports:
      - "9000:9000"
    environment:
      - ASR_MODEL=base.en
    # Optional: deploy whisper.cpp later for lightweight inference

  piper:
    image: rhasspy/piper
    container_name: piper
    ports:
      - "10200:10200"
    command: ["--voice", "en-us-ryan-low"]
    # Ready for real-time TTS responses from n8n workflows

  # traefik:
  #   image: traefik:v2.9
  #   container_name: traefik
  #   ports:
  #     - "80:80"
  #     - "443:443"
  #     - "8080:8080" # Dashboard
  #   command:
  #     - "--api.dashboard=true"
  #     - "--providers.docker=true"
  #     - "--entrypoints.web.address=:80"
  #     - "--entrypoints.websecure.address=:443"
  #     - "--certificatesresolvers.myresolver.acme.tlschallenge=true"
  #     - "--certificatesresolvers.myresolver.acme.email=YOUR_EMAIL"
  #     - "--certificatesresolvers.myresolver.acme.storage=/letsencrypt/acme.json"
  #   volumes:
  #     - "/var/run/docker.sock:/var/run/docker.sock:ro"
  #     - "traefik_letsencrypt:/letsencrypt"
  #   networks:
  #     - web

volumes:
  n8n_data:
  postgres_data:
  ollama_data:
  qdrant_storage:
  # traefik_letsencrypt:

# networks:
#   web:
#     driver: bridge

# To launch:
# docker compose up -d
# Access services:
# - n8n: http://localhost:5678
# - Whisper ASR: http://localhost:9000/asr
# - Piper TTS: http://localhost:10200/api/tts
# - Ollama: http://localhost:11434
# - Qdrant: http://localhost:6333
